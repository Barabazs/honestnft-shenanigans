{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rarity data loaded from: ../metadata/rarity_data/quaks_raritytools.csv\n",
      "Getting minting data for quaks\n",
      "getting page 1 ...\n",
      "Sucessfuly received page 1 of 17\n",
      "getting page 2 ...\n",
      "Sucessfuly received page 2 of 17\n",
      "getting page 3 ...\n",
      "Sucessfuly received page 3 of 17\n",
      "getting page 4 ...\n",
      "Sucessfuly received page 4 of 17\n",
      "getting page 5 ...\n",
      "Sucessfuly received page 5 of 17\n",
      "getting page 6 ...\n",
      "Sucessfuly received page 6 of 17\n",
      "getting page 7 ...\n",
      "Sucessfuly received page 7 of 17\n",
      "getting page 8 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 8 ...\n",
      "Sucessfuly received page 8 of 17\n",
      "getting page 9 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 9 ...\n",
      "Sucessfuly received page 9 of 17\n",
      "getting page 10 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 10 ...\n",
      "Sucessfuly received page 10 of 17\n",
      "getting page 11 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 11 ...\n",
      "Sucessfuly received page 11 of 17\n",
      "getting page 12 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 12 ...\n",
      "Sucessfuly received page 12 of 17\n",
      "getting page 13 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 13 ...\n",
      "Sucessfuly received page 13 of 17\n",
      "getting page 14 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 14 ...\n",
      "Sucessfuly received page 14 of 17\n",
      "getting page 15 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 15 ...\n",
      "Sucessfuly received page 15 of 17\n",
      "getting page 16 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 16 ...\n",
      "Sucessfuly received page 16 of 17\n",
      "getting page 17 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 17 ...\n",
      "Sucessfuly received page 17 of 17\n",
      "--- 59.3 seconds ---\n",
      "finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6606/1260971106.py:114: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['time'] = df['time'].str.replace('.000Z', '')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A Moralis API Key is required.\n",
    "The free tier includes one and is enough to get minting data.\n",
    "\n",
    "AVAILABLE CHAINS:\n",
    "    eth, ropsten, rinkeby, goerli, kovan,\n",
    "    polygon, mumbai, bsc, bsc testnet,\n",
    "    avalanche, avalanche testnet, fantom\n",
    "\n",
    "FOLDER: (Location of the collection rarity csv)\n",
    "    1 - default HonestNFT folder\n",
    "    2 - 'from_raritytools.ipynb' folder\n",
    "\"\"\"\n",
    "\n",
    "COLLECTION_NAME = \"quaks\"\n",
    "CONTRACT = \"0x07bbdaf30e89ea3ecf6cadc80d6e7c4b0843c729\"\n",
    "CHAIN = \"eth\"\n",
    "FOLDER = 1\n",
    "\n",
    "KEEP_ALL_DATA = False  # set to TRUE to keep the raw JSON on disk\n",
    "\n",
    "MAX_RESULTS = 500  # max results per request\n",
    "TIME_DELTA = 0.1  # time to wait between sucessful calls\n",
    "TIME_DELTA_2 = 5  # time to wait after API throttling message\n",
    "\n",
    "\n",
    "def get_mintdata(COLLECTION_NAME, CONTRACT, CHAIN, FOLDER):\n",
    "    import os\n",
    "    import requests\n",
    "    import json\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "    load_dotenv(find_dotenv(\".env\"))\n",
    "    MORALIS_API_KEY = os.getenv(\"MORALIS_API_KEY\")\n",
    "\n",
    "    if FOLDER == 1:\n",
    "        RARITY_CSV = f\"../metadata/rarity_data/{COLLECTION_NAME}_raritytools.csv\"\n",
    "    else:\n",
    "        RARITY_CSV = f\"../metadata/from_raritytools/{COLLECTION_NAME}/{COLLECTION_NAME}_raritytools.csv\"\n",
    "\n",
    "    print(f\"Rarity data loaded from: {RARITY_CSV}\")\n",
    "    RARITY_DB = pd.read_csv(RARITY_CSV)\n",
    "\n",
    "    headers = {\"Content-type\": \"application/json\", \"x-api-key\": MORALIS_API_KEY}\n",
    "\n",
    "    print(f\"Getting minting data for {COLLECTION_NAME}\")\n",
    "    more_results = True\n",
    "    page = 0\n",
    "    start_time = time.time()\n",
    "    all_data = list()  # empty list to store data as it comes\n",
    "    while more_results:\n",
    "        url = \"https://deep-index.moralis.io/api/v2/nft/{}/transfers?chain={}&format=decimal&offset={}&limit={}\".format(\n",
    "            CONTRACT, CHAIN, page * 500, MAX_RESULTS\n",
    "        )\n",
    "        print(f\"getting page {page + 1} ...\")\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(\n",
    "                \"Sucessfuly received page {} of {}\".format(\n",
    "                    page + 1, int(1 + response.json()[\"total\"] / MAX_RESULTS)\n",
    "                )\n",
    "            )\n",
    "            PATH = f\"../minting_data/{COLLECTION_NAME}/{page * MAX_RESULTS}.json\"\n",
    "\n",
    "            # add new data to existing list\n",
    "            all_data.extend(response.json()[\"result\"])\n",
    "\n",
    "            page += 1\n",
    "\n",
    "            # if results in this response is less than MAX_RESULTS then it's the last page\n",
    "            if len(response.json()[\"result\"]) < MAX_RESULTS:\n",
    "                more_results = False\n",
    "            else:\n",
    "                time.sleep(TIME_DELTA)\n",
    "\n",
    "        elif response.status_code in [429, 503, 520]:\n",
    "            print(\n",
    "                f\"Got a {response.status_code} response from the server. Waiting {TIME_DELTA_2} seconds and retrying\"\n",
    "            )\n",
    "            time.sleep(TIME_DELTA_2)\n",
    "\n",
    "        else:\n",
    "            print(f\"status_code = {response.status_code}\")\n",
    "            print(\"Received a unexpected error from Moralis API. Closing process.\")\n",
    "            more_results = False\n",
    "\n",
    "    # Save full json data to one master file\n",
    "    if KEEP_ALL_DATA:\n",
    "        folder = f\"../minting_data/{COLLECTION_NAME}/\"\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "\n",
    "        PATH = f\"../minting_data/{COLLECTION_NAME}/{COLLECTION_NAME}.json\"\n",
    "        with open(PATH, \"w\") as destination_file:\n",
    "            json.dump(all_data, destination_file)\n",
    "\n",
    "    df = json_normalize(all_data)\n",
    "    # remove non minting rows\n",
    "    df = df.loc[df[\"from_address\"] == \"0x0000000000000000000000000000000000000000\"]\n",
    "\n",
    "    # make sure token_id is an integer\n",
    "    df[\"token_id\"] = df[\"token_id\"].astype(int)\n",
    "\n",
    "    # add rarity rank to minting data\n",
    "    df = df.merge(RARITY_DB, left_on=\"token_id\", right_on=\"TOKEN_ID\")\n",
    "\n",
    "    # discard unwanted columns\n",
    "    df = df[\n",
    "        [\n",
    "            \"transaction_hash\",\n",
    "            \"to_address\",\n",
    "            \"token_id\",\n",
    "            \"from_address\",\n",
    "            \"Rank\",\n",
    "            \"block_timestamp\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # get matching columns names to HonestNFT csv format\n",
    "    df.columns = [\"txid\", \"to_account\", \"TOKEN_ID\", \"current_owner\", \"rank\", \"time\"]\n",
    "\n",
    "    # clean 'time' field to make it compatible with the csv produced by 'find_minting_data.ipynb'\n",
    "    df[\"time\"] = df[\"time\"].str.replace(\".000Z\", \"\")\n",
    "\n",
    "    df.to_csv(f\"../minting_data/{COLLECTION_NAME}_minting.csv\")\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (round(time.time() - start_time, 1)))\n",
    "    print(\"finished\")\n",
    "\n",
    "\n",
    "get_mintdata(COLLECTION_NAME, CONTRACT, CHAIN, FOLDER)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
